



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>penambanganweb : crawler dan Kmean</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#2196f3">
      
    
    
      <script src="assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="blue" data-md-color-accent="light-blue">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#tutorial-crawler-menggunakan-scrapy-dan-clushtering-dokumen-menggunakan-k-mean" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="." title="My Docs" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              zainal arif
            </span>
            <span class="md-header-nav__topic">
              Tutorial crawler menggunakan scrapy dan clushtering dokumen menggunakan k-mean
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="." title="My Docs" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    My Docs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        **Tutorial crawler menggunakan scrapy dan clushtering dokumen menggunakan k-mean**
      </label>
    
    <a href="." title="**Tutorial crawler menggunakan scrapy dan clushtering dokumen menggunakan k-mean**" class="md-nav__link md-nav__link--active">
      Tutorial crawler menggunakan scrapy dan clushtering dokumen menggunakan k-mean
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-scrapy" title="1. scrapy" class="md-nav__link">
    1. scrapy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-pemasangan-scrapy" title="1.1 pemasangan scrapy" class="md-nav__link">
    1.1 pemasangan scrapy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-membuat-project-scrapy" title="1.2 membuat project scrapy" class="md-nav__link">
    1.2 membuat project scrapy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#13-mengatur-domain" title="1.3 mengatur domain" class="md-nav__link">
    1.3 mengatur domain
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#14-mengatur-startpage" title="1.4 mengatur startPage" class="md-nav__link">
    1.4 mengatur startPage
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#15-mengatur-rules" title="1.5 mengatur rules" class="md-nav__link">
    1.5 mengatur rules
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#16-mengatur-data-yang-akan-diambil" title="1.6 mengatur data yang akan diambil" class="md-nav__link">
    1.6 mengatur data yang akan diambil
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#17-crawl-data" title="1.7 crawl data" class="md-nav__link">
    1.7 crawl data
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-migrasi-csv-ke-sqlite" title="2. migrasi csv ke sqlite" class="md-nav__link">
    2. migrasi csv ke sqlite
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-clushtering-data" title="3. clushtering data" class="md-nav__link">
    3. clushtering data
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#31-koneksi-database" title="3.1 koneksi database" class="md-nav__link">
    3.1 koneksi database
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#32-menjadikan-data-terstruktur" title="3.2 menjadikan data  terstruktur" class="md-nav__link">
    3.2 menjadikan data  terstruktur
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#33-vsm" title="3.3 vsm" class="md-nav__link">
    3.3 vsm
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#34-kmean" title="3.4 kmean" class="md-nav__link">
    3.4 kmean
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-scrapy" title="1. scrapy" class="md-nav__link">
    1. scrapy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-pemasangan-scrapy" title="1.1 pemasangan scrapy" class="md-nav__link">
    1.1 pemasangan scrapy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-membuat-project-scrapy" title="1.2 membuat project scrapy" class="md-nav__link">
    1.2 membuat project scrapy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#13-mengatur-domain" title="1.3 mengatur domain" class="md-nav__link">
    1.3 mengatur domain
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#14-mengatur-startpage" title="1.4 mengatur startPage" class="md-nav__link">
    1.4 mengatur startPage
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#15-mengatur-rules" title="1.5 mengatur rules" class="md-nav__link">
    1.5 mengatur rules
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#16-mengatur-data-yang-akan-diambil" title="1.6 mengatur data yang akan diambil" class="md-nav__link">
    1.6 mengatur data yang akan diambil
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#17-crawl-data" title="1.7 crawl data" class="md-nav__link">
    1.7 crawl data
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-migrasi-csv-ke-sqlite" title="2. migrasi csv ke sqlite" class="md-nav__link">
    2. migrasi csv ke sqlite
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-clushtering-data" title="3. clushtering data" class="md-nav__link">
    3. clushtering data
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#31-koneksi-database" title="3.1 koneksi database" class="md-nav__link">
    3.1 koneksi database
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#32-menjadikan-data-terstruktur" title="3.2 menjadikan data  terstruktur" class="md-nav__link">
    3.2 menjadikan data  terstruktur
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#33-vsm" title="3.3 vsm" class="md-nav__link">
    3.3 vsm
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#34-kmean" title="3.4 kmean" class="md-nav__link">
    3.4 kmean
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="tutorial-crawler-menggunakan-scrapy-dan-clushtering-dokumen-menggunakan-k-mean"><strong>Tutorial crawler menggunakan scrapy dan clushtering dokumen menggunakan k-mean</strong></h1>
<h2 id="1-scrapy"><strong>1. scrapy</strong></h2>
<blockquote>
<p>scrapy menurut <a href="https://en.wikipedia.org/wiki/Scrapy">wikipedia</a> Scrapy (/ˈskreɪpi/ skray-pee) adalah framework gratis dan opensource python yang di desain untuk <em>web scrapping</em> dan juga bisa digunakan untuk mengekstrak data menggunakan API maupun seperti web crawler lain pada umumnya , saat ini scrapy dikelola oleh Scrapinghub Ltd.</p>
</blockquote>
<h2 id="11-pemasangan-scrapy">1.1 pemasangan scrapy</h2>
<p>pastikan sudah memasang <a href="https://www.python.org/">python</a> versi terbaru dan melakukan centang pada pip </p>
<p><img alt="gambar 1" src="assets\images\gambar 1.PNG" /></p>
<p>agar bisa menggunakan perintah pip di cmd</p>
<p>setelah memasang python , maka install scrapy mengggunakan perintah <code>pip install scrapy</code></p>
<p>jika gagal menginstall scrapy silahkan install terlebih dahulu yang diminta scrapy bisa saja .net framework ataupun visual studio 14 tergantung keadaan komputer masing masing </p>
<h2 id="12-membuat-project-scrapy">1.2 membuat project scrapy</h2>
<p>membuat project scrapy pertama buka folder tempat kita akan membuat project , disana tekan shift+klikKanan kemudian pilih  <strong>open command window here</strong> maka  akan muncul cmd dan ketik disana <code>scrapy startproject olx</code> seperti berikut</p>
<p><img alt="1555651284646" src="assets\images\1555651284646.png" /></p>
<p>maka akan tergenerate project scrapy seperti berikut <img alt="1555651471600" src="assets\images\1555651471600.png" /></p>
<p>setelah itu gunakan perintah <code>cd tugasAkhir</code> untuk berpindah directory ke tugasAkhir dan jalankan perintah berikut untuk membuat crawlernya <code>scrapy genspider tugasAkhir pta.trunojoyo.ac.id</code> </p>
<h2 id="13-mengatur-domain">1.3 mengatur domain</h2>
<p>domain digunakan untuk membatasi lingkup situs yang dijelajahi oleh crawler ,</p>
<p>buka file <strong>tugasAkhir.py</strong> yang berada di folder spiders lalu atur <code>allowed_domains = ['pta.trunojoyo.ac.id']</code> untuk membatasi link yang dijelajahi crawler tetap berada di domain <code>pta.trunojoyo.ac.id</code></p>
<h2 id="14-mengatur-startpage">1.4 mengatur startPage</h2>
<p>startpage berguna memberi tahu  crawler link awal tempat dia mulai menjelajah seperti berikut <code>start_urls = ['https://pta.trunojoyo.ac.id/welcome/index/4']</code> isi di dalam list bisa lebih dari 1 link , saya mulai melakukan crawl mulai halaman 4 karena halaman 1-3 strukturnya berbeda dengan halaman 4-seterusnya sehingga agar mudah saya crawl mulai halaman 4 </p>
<h2 id="15-mengatur-rules">1.5 mengatur rules</h2>
<p>tambahkan rule berikut di baris setelah startUrl</p>
<pre><code class="python">rules = (
 Rule(LinkExtractor(allow=(), restrict_xpaths=('//*[@id=&quot;end&quot;]/ol/li[8]/a',)),callback=&quot;parse_item&quot;,follow=True),)
</code></pre>

<p>rule ini bisa diartikan untuk link di alamat xpath <code>'//*[@id="end"]/ol/li[8]/a'</code> di parse ke fungsi parse_item , dan <code>follow=True</code> berarti di dalam halaman link tadi jika ada alamat xpath <code>'//*[@id="end"]/ol/li[8]/a'</code> maka jelajahi juga sampai alamat xpath tersebut tidak exist</p>
<p>xpath bisa dicopy melalui inspect element</p>
<p>untuk mengenal lebih dalam xpath anda bisa membaca beberapa artikel berikut:</p>
<ul>
<li>https://medium.com/@achmadsyah/tentang-aku-kamu-xpath-f15f67b98733</li>
<li>https://id.wikipedia.org/wiki/XPath</li>
</ul>
<h2 id="16-mengatur-data-yang-akan-diambil">1.6 mengatur data yang akan diambil</h2>
<p>setelah rule mengatur tombol mana yang mengarah ke pagging berikutnya maka link content tiap pagging harus di extract  , disini saya mengextraknya berdasarkan css <code>'a.gray.button::attr(href)'</code>  digambar berikut bisa dilihat css yang menuju halaman detail</p>
<p><img alt="1555660038677" src="assets\images\1555660038677.png" /></p>
<p>css tadi di extract di method parse_item kemudian tiap link yang didapat di parse ke method <code>parse_detail_page</code></p>
<pre><code class="python">def parse_item(self, response):
        print('Processing..' + response.url)

        item_links = response.css('a.gray.button::attr(href)').extract()
        # print(item_links)
        for a in item_links:
            print(&quot;membaca artikel .... &quot;+a)
            yield scrapy.Request(a, callback=self.parse_detail_page)
</code></pre>

<p>setelah link menuju halaman detail di parse maka berikutnya adalah mengextract data yang ingin diambil melalui method <code>parse_detail_page</code>  , </p>
<p>cara mengextract judul penulis sebenarnya sama saja menggunakan css maupun xpath , tapi direkomendaskan menggunakan css jika struktur web tersebut mendukung css yang tertata , sedangkan imbuhan <code>.extract()[0]</code> digunakan untuk ambil data didalamnya karena return dari <code>response.css()</code> maupun <code>response.xpath()</code> adalah list, </p>
<p>jika ingin berexperiment dengan xpath ataupun css agar data yang diambil sesuai keinginan maka gunakan perintah <code>scrapy shell https://pta.trunojoyo.ac.id/welcome/index/4</code> di cmd dan uji xpath atau  css kalian dengan perintah <code>response.css()</code> ataupun <code>response.xpath()</code> jika data yang ditampilkan sesuai maka gunakan xpath ataupun css tersebut</p>
<p>di method ini selain mengekstrack data juga pembersihan data ,beberapa data memilikikata yang tidak ingin saya ambil seperti <strong>Penulis : nama</strong> sedangkan yang ingin saya ambil hanya <strong>nama</strong> saja maka saya melakukan replace di variable penulis</p>
<p>kemudian item adalah data yang akan kita simpan ke csv atributnya apa saja tinggal kita sesuaikan</p>
<pre><code class="python"> def parse_detail_page(self, response):
        judul = response.css('a.title::text').extract()[0].strip()
        penulis = response.xpath('//*[@id=&quot;content_journal&quot;]/ul/li/div[2]/div[1]/span/text()').extract()[0]
        abstrak = response.xpath('//*[@id=&quot;content_journal&quot;]/ul/li/div[4]/div[2]/p/text()').extract()[0]
        url = response.url

        penulis = penulis.replace('Penulis : ', ''

        item = ItemTugasAkhir()
        item['judul'] = judul
        item['penulis'] = penulis
        item['abstrak'] = abstrak
        item['url'] = url
        yield item
</code></pre>

<p>jangan lupa juga meng edit <strong>items.py</strong> agar field yang di parse oleh <code>parse_detail_page</code> sesuai dengan parameter di  <code>ItemTugasAkhir()</code> </p>
<pre><code class="python">import scrapy


class PtatrunojoyoItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    pass

class ItemTugasAkhir(scrapy.Item):
    # define the fields for your item here like:
    judul = scrapy.Field()
    penulis = scrapy.Field()
    abstrak = scrapy.Field()
    url = scrapy.Field()
</code></pre>

<h2 id="17-crawl-data">1.7 crawl data</h2>
<p>untuk crawling data bisa menggunakan perintah <code>scrapy crawl --nolog TugasAkhir -o data.csv -t csv</code>  data hasil crawling akan disimpan di data.csv</p>
<p>proses crawling seperti berikut tunggu hingga selesai</p>
<p><img alt="1555664423165" src="assets\images\1555664423165.png" /></p>
<p>setelah selesai data akan menjadi seperti berikut :</p>
<p><img alt="1555674935243" src="assets\images\1555674935243.png" /></p>
<h2 id="2-migrasi-csv-ke-sqlite">2. migrasi csv ke sqlite</h2>
<p>saya menggunakan tools <a href="https://sqlitebrowser.org/">db browser for sqlite</a> disini sangat mudah  klik new database ketika muncul  pop up untuk mengisi field apa saja yang ada di db close saja </p>
<p><img alt="1555675137009" src="assets\images\1555675137009.png" /></p>
<p>setelah itu di menu file pilih import table form csv lali pilih file csv tadi dan lakukan proses migrasi</p>
<p><img alt="1555675466884" src="assets\images\1555675466884.png" /></p>
<p>tunggu hingga proses selesai </p>
<p><img alt="1555675509234" src="assets\images\1555675509234.png" /></p>
<h2 id="3-clushtering-data">3. clushtering data</h2>
<p>dalam step ini diasumsikan data telah menjadi sqlite dengan struktur seperti ini</p>
<p><img alt="1555681519832" src="assets\images\1555681519832.png" /></p>
<h2 id="31-koneksi-database">3.1 koneksi database</h2>
<p>saya menggunakan library sqlite3 untuk melakukan koneksi dengan db , code untuk melakukan koneksi sebagai berikut</p>
<pre><code class="python">import sqlite3 
def koneksi(db_file):
    try:
        conn = sqlite3.connect(db_file)
        return conn
    except Error as e:
        print(e)
    return None

database = &quot;data DUMMY.sqlite&quot;
#pastikan nama file database benar dan berada 1 directory dengan file python
conn = koneksi(database)
</code></pre>

<h2 id="32-menjadikan-data-terstruktur">3.2 menjadikan data  terstruktur</h2>
<p>field yang akan diolah adalah field  deskripsi , untuk merubahnya menjadi data terstruktur ada beberapa step yang harus dilakukan yaitu :</p>
<ul>
<li>lowercase </li>
<li>stopword</li>
<li>stemming</li>
<li>tokenisasi</li>
</ul>
<p>seperti contoh program berikut :</p>
<pre><code class="python">def keDataTerstruktur(text):
    text = lowerCase(text)
    text = stopword(text)
    text = stemming(text)
    text = tokenisasi(text)
    return text 
</code></pre>

<p>adapun isi dari masing-masing fungsi dan penjelasnnya  sebagai berikut :</p>
<pre><code class="python">def lowerCase(text):
    text = text.lower()
    #mengubah keseluruhan text menjadi lowercase
    return text
def stopword(text):
    # Ambil Stopword bawaan
    stop_factory = StopWordRemoverFactory().get_stop_words()
    print(stop_factory)
    more_stopword = ['diatur', 'perjodohan']

    # Merge / menggabungkan stopword bawaan dan tambahan
    data = stop_factory + more_stopword

    dictionary = ArrayDictionary(data)
    str = StopWordRemover(dictionary)

    hasil = str.remove(text)
    # print(hasil)

    return hasil
def stemming(text):
    # create stemmer
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    #melakukan stemming , atau mengubah kata menjadi kata dasar
    hasil = stemmer.stem(text)
    return hasil
def tokenisasi(text):

    #memecah kata berdasarkan spasi
    text = text.split()
    return text

</code></pre>

<p>setelah membuat code yang mengubah suatu text menjadi data terstruktur  sekarang adalah cara mengambi data dari database dan memasukkannya ke dalam fungsi <code>def keDataTerstruktur(text)</code> tadi , dengan menggunakan koneksi <code>conn</code> yang telah ada kita ambil datanya dengan menggunakan code</p>
<pre><code class="python">def prosesDataTerstruktur(conn):
    cur = conn.cursor()
    #execute query select
    cur.execute(&quot;SELECT * FROM data&quot;)
    rows = cur.fetchall()
    print(&quot;prosesDataTerstruktur&quot;, end=&quot;&quot;, flush=True)
    for row in rows:
        #mengakses variable daftaKata
        global daftarKata
        #row[0] atau deskripsi di parse ke fungsi keDataTerstruktur()
        text = keDataTerstruktur(row[0])
        #mengumpulkan kata dari semua dokumen digunakan nanti di vsm
        daftarKata = daftarKata.union(set(text))

        text = ' '.join(text)
        #proses menyimpan row[3] adalah nim yang dijadikan primary key
        simpanDataTerstruktur(conn , text ,row[3])

        print(&quot;. &quot;, end=&quot;&quot;, flush=True)
    print(&quot;done&quot;)
</code></pre>

<p>data yang telah menjadi terstruktur akan disimpann di database mennggunakan kode berikut:</p>
<pre><code class="python">#memastikan field dataTerstruktur ada dalam database
def cekKolomDataTerstruktur(conn):
    cur = conn.cursor()
    cur.execute(&quot;PRAGMA table_info(data);&quot;)
    rows = cur.fetchall()
    for row in rows:
        # print(row[1])
        if row[1] =='dataTerstruktur':
            return True
            end
    cur.execute(&quot;ALTER TABLE data ADD COLUMN dataTerstruktur TEXT;&quot;)
    return False
def simpanDataTerstruktur(conn , text , url):
    task = (str(text) , str(url))
    sql = &quot;UPDATE data SET dataTerstruktur = ? WHERE url = ?&quot;
    cur = conn.cursor()
    cur.execute(sql,task)
    return cur.lastrowid
</code></pre>

<p>maka file database setelah dilakukan proses ini akan menjadi </p>
<p><img alt="1555745362681" src="assets\images\1555745362681.png" /></p>
<h2 id="33-vsm">3.3 vsm</h2>
<p>setelah menjalankan method <code>def prosesDataTerstruktur(conn):</code> maka variable globa <code>daftarKata</code> akan berisi seluruh kata (yang telah menjadi data terstruktur) dari seluruh dokumen yang ada di database maka proses vsm adalah menghitung berapa banyak masing masing kata terkandung di dalam dokumen , metode untuk menghitung berapa jumlah kata di sebuah text sebagai berikut</p>
<pre><code class="python">def keVsm(text):
    global daftarKata
    text = text.split()
    tmp = []

    #melakukan perulangan sepanjang 'daftarKata' untukmenghitung masing masing kata ada berapa pada 'text'
    for kata in daftarKata:
        jumlahKata = text.count(kata)
        tmp = tmp+[jumlahKata]

    print('panjang vsm = ', len(tmp))
    return tmp
</code></pre>

<p>berikutnya hanya perlu mengambil data terstruktur di database dan parse ke method  <code>def keVsm(text):</code></p>
<pre><code class="python">def prosesVsm (conn):
    cur = conn.cursor()
    cur.execute(&quot;SELECT * FROM data&quot;)
    rows = cur.fetchall()
    print(&quot;prosesVsm&quot;, end=&quot;&quot;, flush=True)
    for row in rows:
        #row[4] yaitu data terstruktur di parse ke method keVsm()
        kata = keVsm(row[4])
        #hasil return adalah list maka harus menggunakan fungsi join agar menjadi string , sebernarnya bisa menggunakan variable lain selain kata , tapi karena variable kata tidak digunakan lagi maka masukkan saja ke variable itu 
        kata = ' '.join(str(x) for x in kata)
        #fungsi penyimpanan vsm
        simpanDataVsm(conn , kata ,row[3])
        print(&quot;. &quot;, end=&quot;&quot;, flush=True)
    print(&quot;done&quot;)
</code></pre>

<p>untuk fungsi penyimpanan dan cek field kurang lebih sama dengan penyimpanan dan cek field data terstruktur hanya perlu penyesuaian sedikit</p>
<h2 id="34-kmean">3.4 kmean</h2>
<p>step dasar kmean sebagai berikut</p>
<ol>
<li>tentukan jumlah clushter</li>
<li>random pusat clushter sebanyak jumlah clushter</li>
<li>kemudian cari jarak masing-masing pusat clushter ke setiap anggota  lain</li>
<li>jarak terdekat adalah  anggota clushter tersebut</li>
<li>hitung rata-rata seluruh anggota masing-masing clushter , rata-rata tsb adalah pusat clushter baru</li>
<li>ulangi langkah 3,4,5 sampai tidak ada anggota yang berpindah clushter atau pusat clushter == pusat clushter baru</li>
</ol>
<p>jika ditulis dalam code versi saya akan seperti ini</p>
<pre><code class="python">def kmean(conn,k,url,vsm):
    #1. k adalah jumlah clushter
    global centerCluster , clushter , centerDataCluster
    newClushter=[]
    iterasi = 0
    #2.random pusat clushter
    centerCluster = list(random(k,len(vsm)))
    print(&quot;centerCluster = &quot;,centerCluster)
    for x in range(0,k):  
        centerDataCluster.append(getCenterData(conn,centerCluster[x]))
    while(True): 
        iterasi+=1

        for dataKe in vsm:
            #3. tmp jarak menampung jarak data ke masing masing center
            tmpJarak=[]
            jcenter=0
            for centerKe in centerDataCluster:
                tmpJarak.append(manhattanDistance(dataKe,centerKe))
            #4. kemudian diambil jarak terendah
            cls = tmpJarak.index(min(tmpJarak))
            newClushter.append(cls+1)
        # pengecekan apakah ada perubahan anggota clushter
        if(newClushter == clushter):
            break
        else:
            clushter = newClushter
            newClushter = []

            print(clushter)
            NewCenter = []
            for x in range(1,k+1):          
                tmp = [0] * len(vsm[0])
                jumlah = 0
                for y in clushter:
                    if(y==x):

                        tmp = aPlusB(tmp,vsm[y]) 
                        jumlah+=1
                #5. hitung rata-rata pusat clushter baru
                NewCenter.append(mean(tmp,jumlah))
            centerDataCluster = NewCenter
    return clushter
</code></pre>

<p>hasil kmean akan seperti berikut</p>
<p><img alt="1555754273918" src="assets\images\1555754273918.png" /></p>
<p>sekian penjelasan saya kurang lebihnya mohon maaf   , terimakasih</p>
<p>full code ada di repo berikut  : <a href="https://github.com/ariefzzz/penambanganweb-crawler-Kmean/tree/master/source">penambanganweb-crawler-Kmean</a></p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="assets/javascripts/application.39abc4af.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"."}})</script>
      
    
  </body>
</html>