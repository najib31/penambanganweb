{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tutorial crawler menggunakan scrapy dan clushtering dokumen menggunakan k-mean 1. scrapy scrapy menurut wikipedia Scrapy (/\u02c8skre\u026api/ skray-pee) adalah framework gratis dan opensource python yang di desain untuk web scrapping dan juga bisa digunakan untuk mengekstrak data menggunakan API maupun seperti web crawler lain pada umumnya , saat ini scrapy dikelola oleh Scrapinghub Ltd. 1.1 install scrapy install python versi terbaru dan melakukan centang pada pip agar bisa menggunakan perintah pip di cmd setelah memasang python , maka install scrapy mengggunakan perintah pip install scrapy jika gagal menginstall scrapy silahkan install terlebih dahulu yang diminta scrapy bisa saja .net framework ataupun visual studio 14 tergantung spesifikasi komputer masing masing. 1.2 membuat project scrapy membuat project scrapy pertama buka folder tempat kita akan membuat project , disana tekan shift+klikKanan kemudian pilih open command window here maka akan muncul cmd dan ketik disana scrapy startproject olx seperti berikut \u200b maka akan muncul project scrapy seperti berikut setelah itu gunakan perintah cd tugasAkhir untuk berpindah directory ke tugasAkhir dan jalankan perintah berikut untuk membuat crawlernya scrapy genspider tugasAkhir maduracorner.com 1.3 mengatur domain domain digunakan untuk membatasi lingkup situs yang dijelajahi oleh crawler , buka file productInfo.py yang berada di folder spiders lalu atur allowed_domains = ['maduracorner.com'] untuk membatasi link yang dijelajahi crawler tetap berada di domain maduracorner.com 1.4 mengatur startPage startpage berguna memberi tahu crawler link awal tempat dia mulai menjelajah seperti berikut start_urls = ['https://www.maduracorner.com/category/berita-terkini/page/6/'] isi di dalam list bisa lebih dari 1 link , saya mulai melakukan crawl mulai halaman 6 karena halaman 1-3 strukturnya berbeda dengan halaman 6-seterusnya sehingga agar mudah saya crawl mulai halaman 6 1.5 mengatur rules tambahkan rule berikut di baris setelah startUrl rules = ( Rule(LinkExtractor(allow=(), restrict_xpaths=('//*[@id=\"end\"]/ol/li[8]/a',)),callback=\"parse_item\",follow=True),) rule ini bisa diartikan untuk link di alamat xpath '//*[@id=\"end\"]/ol/li[8]/a' di parse ke fungsi parse_item , dan follow=True berarti di dalam halaman link tadi jika ada alamat xpath '//*[@id=\"end\"]/ol/li[8]/a' maka jelajahi juga sampai alamat xpath tersebut tidak exist xpath bisa dicopy melalui inspect element 1.6 mengatur data yang akan diambil setelah rule mengatur tombol mana yang mengarah ke pagging berikutnya maka link content tiap pagging harus di extract , disini saya mengextraknya berdasarkan css 'a.gray.button::attr(href)' digambar berikut bisa dilihat css yang menuju halaman detail css tadi di extract di method parse_item kemudian tiap link yang didapat di parse ke method parse_detail_page def parse_item(self, response): print('Processing..' + response.url) item_links = response.css('.entry-title>a::attr(href)').extract() print(item_links) for a in item_links: print(\"membaca artikel .... \"+a) # if(a == 'https://www.maduracorner.com/category/berita-terkini/page/6/'): # break # else: yield scrapy.Request(a, callback=self.parse_detail_page) setelah link menuju halaman detail di parse maka berikutnya adalah mengextract data yang ingin diambil melalui method parse_detail_page , cara mengextract judul penulis sebenarnya sama saja menggunakan css maupun xpath , tapi direkomendaskan menggunakan css jika struktur web tersebut mendukung css yang tertata , sedangkan imbuhan .extract()[0] digunakan untuk ambil data didalamnya karena return dari response.css() maupun response.xpath() adalah list, jika ingin berexperiment dengan xpath ataupun css agar data yang diambil sesuai keinginan maka gunakan perintah scrapy shell https://www.maduracorner.com/category/berita-terkini/page/6 di cmd dan uji xpath atau css kalian dengan perintah response.css() ataupun response.xpath() jika data yang ditampilkan sesuai maka gunakan xpath ataupun css tersebut di method ini selain mengekstrack data juga pembersihan data ,beberapa data memilikikata yang tidak ingin saya ambil seperti Penulis : nama sedangkan yang ingin saya ambil hanya nama saja maka saya melakukan replace di variable penulis kemudian item adalah data yang akan kita simpan ke csv atributnya apa saja tinggal kita sesuaikan def parse_detail_page(self, response): judul = response.css('h1.entry-title.clearfix::text').extract()[0] deskripsi = response.css('div.ktz-content-single').extract()[0] deskripsi = remove_tags(deskripsi) deskripsi.replace('\\n','') deskripsi.replace('\\t','') deskripsi.replace('\\r','') deskripsi = re.sub(r'[^\\w]', ' ', deskripsi) # for x in range(3, 10): # deskripsi =deskripsi + response.css('div.ktz-content-single p:nth-child('+str(x)+')::text').extract()[0] tanggal = response.css('time::text').extract()[0] url = response.url item = productInfo() item['judul'] = judul item['deskripsi'] = deskripsi item['tanggal'] = tanggal item['url'] = url yield item jangan lupa juga meng edit items.py agar field yang di parse oleh parse_detail_page sesuai dengan parameter di ItemproductInfo() import scrapy import re from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor from crawler3Second.items import productInfo from w3lib.html import remove_tags class ProductSpider(CrawlSpider): name = \"product\" allowed_domains = ['www.maduracorner.com'] start_urls = ['https://www.maduracorner.com/category/berita-terkini/page/3/'] 1.7 crawl data untuk crawling data bisa menggunakan perintah scrapy crawl --nolog produckInfo -o data.csv -t csv data hasil crawling akan disimpan di data.csv setelah selesai data akan menjadi seperti berikut : 2. migrasi csv ke sqlite saya menggunakan tools db browser for sqlite disini sangat mudah klik new database ketika muncul pop up untuk mengisi field apa saja yang ada di db close saja setelah itu di menu file pilih import table form csv lali pilih file csv tadi dan lakukan proses migrasi 3. clushtering data dalam step ini diasumsikan data telah menjadi sqlite dengan struktur seperti ini 3.1 koneksi database menggunakan ibrary sqlite3 untuk melakukan koneksi dengan db , code untuk melakukan koneksi sebagai berikut ef koneksi(db_file): try: conn = sqlite3.connect(db_file) return conn except Error as e: print(e) return None 3.2 menjadikan data terstruktur field yang akan diolah adalah field deskripsi , untuk merubahnya menjadi data terstruktur ada beberapa step yang harus dilakukan yaitu : lowercase stopword stemming tokenisasi seperti contoh program berikut : def keDataTerstruktur(text): text = lowerCase(text) text = stopword(text) text = stemming(text) text = tokenisasi(text) return text adapun isi dari masing-masing fungsi dan penjelasnnya sebagai berikut : def lowerCase(text): text = text.lower() #mengubah keseluruhan text menjadi lowercase return text def stopword(text): # Ambil Stopword bawaan stop_factory = StopWordRemoverFactory().get_stop_words() print(stop_factory) more_stopword = ['diatur', 'perjodohan'] # Merge / menggabungkan stopword bawaan dan tambahan data = stop_factory + more_stopword dictionary = ArrayDictionary(data) str = StopWordRemover(dictionary) hasil = str.remove(text) # print(hasil) return hasil def stemming(text): # create stemmer factory = StemmerFactory() stemmer = factory.create_stemmer() #melakukan stemming , atau mengubah kata menjadi kata dasar hasil = stemmer.stem(text) return hasil def tokenisasi(text): #memecah kata berdasarkan spasi text = text.split() return text setelah membuat code yang mengubah suatu text menjadi data terstruktur sekarang adalah cara mengambi data dari database dan memasukkannya ke dalam fungsi def keDataTerstruktur(text) tadi , dengan menggunakan koneksi conn yang telah ada kita ambil datanya dengan menggunakan code def prosesDataTerstruktur(conn): cur = conn.cursor() #execute query select cur.execute(\"SELECT * FROM data\") rows = cur.fetchall() print(\"prosesDataTerstruktur\", end=\"\", flush=True) for row in rows: #mengakses variable daftaKata global daftarKata #row[0] atau deskripsi di parse ke fungsi keDataTerstruktur() text = keDataTerstruktur(row[0]) #mengumpulkan kata dari semua dokumen digunakan nanti di vsm daftarKata = daftarKata.union(set(text)) text = ' '.join(text) #proses menyimpan row[3] adalah nim yang dijadikan primary key simpanDataTerstruktur(conn , text ,row[3]) print(\". \", end=\"\", flush=True) print(\"done\") data yang telah menjadi terstruktur akan disimpann di database mennggunakan kode berikut: #memastikan field dataTerstruktur ada dalam database def cekKolomDataTerstruktur(conn): cur = conn.cursor() cur.execute(\"PRAGMA table_info(data);\") rows = cur.fetchall() for row in rows: # print(row[1]) if row[1] =='dataTerstruktur': return True end cur.execute(\"ALTER TABLE data ADD COLUMN dataTerstruktur TEXT;\") return False def simpanDataTerstruktur(conn , text , url): task = (str(text) , str(url)) sql = \"UPDATE data SET dataTerstruktur = ? WHERE url = ?\" cur = conn.cursor() cur.execute(sql,task) return cur.lastrowid maka file database setelah dilakukan proses ini akan menjadi 3.3 vsm setelah menjalankan method def prosesDataTerstruktur(conn): maka variable globa daftarKata akan berisi seluruh kata (yang telah menjadi data terstruktur) dari seluruh dokumen yang ada di database maka proses vsm adalah menghitung berapa banyak masing masing kata terkandung di dalam dokumen , metode untuk menghitung berapa jumlah kata di sebuah text sebagai berikut def keVsm(text): global daftarKata text = text.split() tmp = [] #melakukan perulangan sepanjang 'daftarKata' untukmenghitung masing masing kata ada berapa pada 'text' for kata in daftarKata: jumlahKata = text.count(kata) tmp = tmp+[jumlahKata] print('panjang vsm = ', len(tmp)) return tmp berikutnya hanya perlu mengambil data terstruktur di database dan parse ke method def keVsm(text): def prosesVsm (conn): cur = conn.cursor() cur.execute(\"SELECT * FROM data\") rows = cur.fetchall() print(\"prosesVsm\", end=\"\", flush=True) for row in rows: #row[4] yaitu data terstruktur di parse ke method keVsm() kata = keVsm(row[4]) #hasil return adalah list maka harus menggunakan fungsi join agar menjadi string , sebernarnya bisa menggunakan variable lain selain kata , tapi karena variable kata tidak digunakan lagi maka masukkan saja ke variable itu kata = ' '.join(str(x) for x in kata) #fungsi penyimpanan vsm simpanDataVsm(conn , kata ,row[3]) print(\". \", end=\"\", flush=True) print(\"done\") untuk fungsi penyimpanan dan cek field kurang lebih sama dengan penyimpanan dan cek field data terstruktur hanya perlu penyesuaian sedikit","title":"Tutorial crawler menggunakan scrapy dan clushtering dokumen menggunakan k-mean"},{"location":"#tutorial-crawler-menggunakan-scrapy-dan-clushtering-dokumen-menggunakan-k-mean","text":"","title":"Tutorial crawler menggunakan scrapy dan clushtering dokumen menggunakan k-mean"},{"location":"#1-scrapy","text":"scrapy menurut wikipedia Scrapy (/\u02c8skre\u026api/ skray-pee) adalah framework gratis dan opensource python yang di desain untuk web scrapping dan juga bisa digunakan untuk mengekstrak data menggunakan API maupun seperti web crawler lain pada umumnya , saat ini scrapy dikelola oleh Scrapinghub Ltd. 1.1 install scrapy install python versi terbaru dan melakukan centang pada pip agar bisa menggunakan perintah pip di cmd setelah memasang python , maka install scrapy mengggunakan perintah pip install scrapy jika gagal menginstall scrapy silahkan install terlebih dahulu yang diminta scrapy bisa saja .net framework ataupun visual studio 14 tergantung spesifikasi komputer masing masing.","title":"1. scrapy"},{"location":"#12-membuat-project-scrapy","text":"membuat project scrapy pertama buka folder tempat kita akan membuat project , disana tekan shift+klikKanan kemudian pilih open command window here maka akan muncul cmd dan ketik disana scrapy startproject olx seperti berikut \u200b maka akan muncul project scrapy seperti berikut setelah itu gunakan perintah cd tugasAkhir untuk berpindah directory ke tugasAkhir dan jalankan perintah berikut untuk membuat crawlernya scrapy genspider tugasAkhir maduracorner.com","title":"1.2 membuat project scrapy"},{"location":"#13-mengatur-domain","text":"domain digunakan untuk membatasi lingkup situs yang dijelajahi oleh crawler , buka file productInfo.py yang berada di folder spiders lalu atur allowed_domains = ['maduracorner.com'] untuk membatasi link yang dijelajahi crawler tetap berada di domain maduracorner.com","title":"1.3 mengatur domain"},{"location":"#14-mengatur-startpage","text":"startpage berguna memberi tahu crawler link awal tempat dia mulai menjelajah seperti berikut start_urls = ['https://www.maduracorner.com/category/berita-terkini/page/6/'] isi di dalam list bisa lebih dari 1 link , saya mulai melakukan crawl mulai halaman 6 karena halaman 1-3 strukturnya berbeda dengan halaman 6-seterusnya sehingga agar mudah saya crawl mulai halaman 6","title":"1.4 mengatur startPage"},{"location":"#15-mengatur-rules","text":"tambahkan rule berikut di baris setelah startUrl rules = ( Rule(LinkExtractor(allow=(), restrict_xpaths=('//*[@id=\"end\"]/ol/li[8]/a',)),callback=\"parse_item\",follow=True),) rule ini bisa diartikan untuk link di alamat xpath '//*[@id=\"end\"]/ol/li[8]/a' di parse ke fungsi parse_item , dan follow=True berarti di dalam halaman link tadi jika ada alamat xpath '//*[@id=\"end\"]/ol/li[8]/a' maka jelajahi juga sampai alamat xpath tersebut tidak exist xpath bisa dicopy melalui inspect element","title":"1.5 mengatur rules"},{"location":"#16-mengatur-data-yang-akan-diambil","text":"setelah rule mengatur tombol mana yang mengarah ke pagging berikutnya maka link content tiap pagging harus di extract , disini saya mengextraknya berdasarkan css 'a.gray.button::attr(href)' digambar berikut bisa dilihat css yang menuju halaman detail css tadi di extract di method parse_item kemudian tiap link yang didapat di parse ke method parse_detail_page def parse_item(self, response): print('Processing..' + response.url) item_links = response.css('.entry-title>a::attr(href)').extract() print(item_links) for a in item_links: print(\"membaca artikel .... \"+a) # if(a == 'https://www.maduracorner.com/category/berita-terkini/page/6/'): # break # else: yield scrapy.Request(a, callback=self.parse_detail_page) setelah link menuju halaman detail di parse maka berikutnya adalah mengextract data yang ingin diambil melalui method parse_detail_page , cara mengextract judul penulis sebenarnya sama saja menggunakan css maupun xpath , tapi direkomendaskan menggunakan css jika struktur web tersebut mendukung css yang tertata , sedangkan imbuhan .extract()[0] digunakan untuk ambil data didalamnya karena return dari response.css() maupun response.xpath() adalah list, jika ingin berexperiment dengan xpath ataupun css agar data yang diambil sesuai keinginan maka gunakan perintah scrapy shell https://www.maduracorner.com/category/berita-terkini/page/6 di cmd dan uji xpath atau css kalian dengan perintah response.css() ataupun response.xpath() jika data yang ditampilkan sesuai maka gunakan xpath ataupun css tersebut di method ini selain mengekstrack data juga pembersihan data ,beberapa data memilikikata yang tidak ingin saya ambil seperti Penulis : nama sedangkan yang ingin saya ambil hanya nama saja maka saya melakukan replace di variable penulis kemudian item adalah data yang akan kita simpan ke csv atributnya apa saja tinggal kita sesuaikan def parse_detail_page(self, response): judul = response.css('h1.entry-title.clearfix::text').extract()[0] deskripsi = response.css('div.ktz-content-single').extract()[0] deskripsi = remove_tags(deskripsi) deskripsi.replace('\\n','') deskripsi.replace('\\t','') deskripsi.replace('\\r','') deskripsi = re.sub(r'[^\\w]', ' ', deskripsi) # for x in range(3, 10): # deskripsi =deskripsi + response.css('div.ktz-content-single p:nth-child('+str(x)+')::text').extract()[0] tanggal = response.css('time::text').extract()[0] url = response.url item = productInfo() item['judul'] = judul item['deskripsi'] = deskripsi item['tanggal'] = tanggal item['url'] = url yield item jangan lupa juga meng edit items.py agar field yang di parse oleh parse_detail_page sesuai dengan parameter di ItemproductInfo() import scrapy import re from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor from crawler3Second.items import productInfo from w3lib.html import remove_tags class ProductSpider(CrawlSpider): name = \"product\" allowed_domains = ['www.maduracorner.com'] start_urls = ['https://www.maduracorner.com/category/berita-terkini/page/3/']","title":"1.6 mengatur data yang akan diambil"},{"location":"#17-crawl-data","text":"untuk crawling data bisa menggunakan perintah scrapy crawl --nolog produckInfo -o data.csv -t csv data hasil crawling akan disimpan di data.csv setelah selesai data akan menjadi seperti berikut :","title":"1.7 crawl data"},{"location":"#2-migrasi-csv-ke-sqlite","text":"saya menggunakan tools db browser for sqlite disini sangat mudah klik new database ketika muncul pop up untuk mengisi field apa saja yang ada di db close saja setelah itu di menu file pilih import table form csv lali pilih file csv tadi dan lakukan proses migrasi","title":"2. migrasi csv ke sqlite"},{"location":"#3-clushtering-data","text":"dalam step ini diasumsikan data telah menjadi sqlite dengan struktur seperti ini","title":"3. clushtering data"},{"location":"#31-koneksi-database-menggunakan-ibrary-sqlite3","text":"untuk melakukan koneksi dengan db , code untuk melakukan koneksi sebagai berikut ef koneksi(db_file): try: conn = sqlite3.connect(db_file) return conn except Error as e: print(e) return None","title":"3.1 koneksi database menggunakan ibrary sqlite3"},{"location":"#32-menjadikan-data-terstruktur","text":"field yang akan diolah adalah field deskripsi , untuk merubahnya menjadi data terstruktur ada beberapa step yang harus dilakukan yaitu : lowercase stopword stemming tokenisasi seperti contoh program berikut : def keDataTerstruktur(text): text = lowerCase(text) text = stopword(text) text = stemming(text) text = tokenisasi(text) return text adapun isi dari masing-masing fungsi dan penjelasnnya sebagai berikut : def lowerCase(text): text = text.lower() #mengubah keseluruhan text menjadi lowercase return text def stopword(text): # Ambil Stopword bawaan stop_factory = StopWordRemoverFactory().get_stop_words() print(stop_factory) more_stopword = ['diatur', 'perjodohan'] # Merge / menggabungkan stopword bawaan dan tambahan data = stop_factory + more_stopword dictionary = ArrayDictionary(data) str = StopWordRemover(dictionary) hasil = str.remove(text) # print(hasil) return hasil def stemming(text): # create stemmer factory = StemmerFactory() stemmer = factory.create_stemmer() #melakukan stemming , atau mengubah kata menjadi kata dasar hasil = stemmer.stem(text) return hasil def tokenisasi(text): #memecah kata berdasarkan spasi text = text.split() return text setelah membuat code yang mengubah suatu text menjadi data terstruktur sekarang adalah cara mengambi data dari database dan memasukkannya ke dalam fungsi def keDataTerstruktur(text) tadi , dengan menggunakan koneksi conn yang telah ada kita ambil datanya dengan menggunakan code def prosesDataTerstruktur(conn): cur = conn.cursor() #execute query select cur.execute(\"SELECT * FROM data\") rows = cur.fetchall() print(\"prosesDataTerstruktur\", end=\"\", flush=True) for row in rows: #mengakses variable daftaKata global daftarKata #row[0] atau deskripsi di parse ke fungsi keDataTerstruktur() text = keDataTerstruktur(row[0]) #mengumpulkan kata dari semua dokumen digunakan nanti di vsm daftarKata = daftarKata.union(set(text)) text = ' '.join(text) #proses menyimpan row[3] adalah nim yang dijadikan primary key simpanDataTerstruktur(conn , text ,row[3]) print(\". \", end=\"\", flush=True) print(\"done\") data yang telah menjadi terstruktur akan disimpann di database mennggunakan kode berikut: #memastikan field dataTerstruktur ada dalam database def cekKolomDataTerstruktur(conn): cur = conn.cursor() cur.execute(\"PRAGMA table_info(data);\") rows = cur.fetchall() for row in rows: # print(row[1]) if row[1] =='dataTerstruktur': return True end cur.execute(\"ALTER TABLE data ADD COLUMN dataTerstruktur TEXT;\") return False def simpanDataTerstruktur(conn , text , url): task = (str(text) , str(url)) sql = \"UPDATE data SET dataTerstruktur = ? WHERE url = ?\" cur = conn.cursor() cur.execute(sql,task) return cur.lastrowid maka file database setelah dilakukan proses ini akan menjadi","title":"3.2 menjadikan data terstruktur"},{"location":"#33-vsm","text":"setelah menjalankan method def prosesDataTerstruktur(conn): maka variable globa daftarKata akan berisi seluruh kata (yang telah menjadi data terstruktur) dari seluruh dokumen yang ada di database maka proses vsm adalah menghitung berapa banyak masing masing kata terkandung di dalam dokumen , metode untuk menghitung berapa jumlah kata di sebuah text sebagai berikut def keVsm(text): global daftarKata text = text.split() tmp = [] #melakukan perulangan sepanjang 'daftarKata' untukmenghitung masing masing kata ada berapa pada 'text' for kata in daftarKata: jumlahKata = text.count(kata) tmp = tmp+[jumlahKata] print('panjang vsm = ', len(tmp)) return tmp berikutnya hanya perlu mengambil data terstruktur di database dan parse ke method def keVsm(text): def prosesVsm (conn): cur = conn.cursor() cur.execute(\"SELECT * FROM data\") rows = cur.fetchall() print(\"prosesVsm\", end=\"\", flush=True) for row in rows: #row[4] yaitu data terstruktur di parse ke method keVsm() kata = keVsm(row[4]) #hasil return adalah list maka harus menggunakan fungsi join agar menjadi string , sebernarnya bisa menggunakan variable lain selain kata , tapi karena variable kata tidak digunakan lagi maka masukkan saja ke variable itu kata = ' '.join(str(x) for x in kata) #fungsi penyimpanan vsm simpanDataVsm(conn , kata ,row[3]) print(\". \", end=\"\", flush=True) print(\"done\") untuk fungsi penyimpanan dan cek field kurang lebih sama dengan penyimpanan dan cek field data terstruktur hanya perlu penyesuaian sedikit","title":"3.3 vsm"}]}